{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start-up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from transformers import pipeline, AutoModel\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma \n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"decapoda-research/llama-7b-hf\"  # Or another suitable model\n",
    "embeddings_model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "chroma_db_path = \"langchain_docs_db\" # Specify where to store the database\n",
    "langchain_docs_url = \"https://python.langchain.com/en/latest/\" # Starting URL "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions Def"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_langchain_docs(url):\n",
    "  \"\"\"Scrapes content from a LangChain documentation page.\"\"\"\n",
    "  response = requests.get(url)\n",
    "  soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "  # Extract title and content (adjust selectors based on page structure)\n",
    "  title = soup.find('h1', class_='page-title').text.strip()\n",
    "  content_elements = soup.select('div.section > *') # Adjust selectors as needed\n",
    "  content = \" \".join([str(element) for element in content_elements]) \n",
    "\n",
    "  # Remove code blocks (optional)\n",
    "  # content = re.sub(r'<pre>.*?</pre>', '', content, flags=re.DOTALL) \n",
    "\n",
    "  return {\"title\": title, \"content\": content}\n",
    "\n",
    "def create_langchain_docs_db(url, embeddings_model, db_path):\n",
    "  \"\"\"Scrapes, embeds, and stores LangChain documentation in a ChromaDB.\"\"\"\n",
    "  # 1. Scrape the documentation\n",
    "  page_data = scrape_langchain_docs(url) \n",
    "\n",
    "  # 2. Split into chunks\n",
    "  text_splitter = RecursiveCharacterTextSplitter(\n",
    "      chunk_size=500,\n",
    "      chunk_overlap=50,\n",
    "      length_function=len,\n",
    "  )\n",
    "  docs = text_splitter.create_documents([page_data['content']])\n",
    "\n",
    "  # 3. Create the database\n",
    "  vectordb = Chroma.from_documents(\n",
    "      docs, \n",
    "      embeddings_model, \n",
    "      persist_directory=db_path\n",
    "  )\n",
    "  vectordb.persist()\n",
    "  print(f\"Created ChromaDB at: {db_path}\")\n",
    "  return vectordb\n",
    "\n",
    "def get_context_from_docs(query, vectordb):\n",
    "    \"\"\"Retrieves relevant context from the LangChain documentation database.\"\"\"\n",
    "    results = vectordb.similarity_search(query, k=3) \n",
    "    return [doc.page_content for doc in results]\n",
    "\n",
    "def answer_question(question, vectordb, generator):\n",
    "  \"\"\"Answers user questions using the language model and RAG.\"\"\"\n",
    "  context = get_context_from_docs(question, vectordb) \n",
    "  prompt = f\"\"\"\n",
    "  Answer the following question based on the provided context:\n",
    "\n",
    "  Context:\n",
    "  {context}\n",
    "\n",
    "  Question: {question}\n",
    "  Answer:\n",
    "  \"\"\"\n",
    "  response = generator(prompt, max_length=200)[0]['generated_text']\n",
    "  return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading LLM and/or embeddings model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = pipeline('text-generation', model=model_name)\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=embeddings_model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create DB vectorisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this once\n",
    "vectordb = create_langchain_docs_db(langchain_docs_url, embeddings, chroma_db_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectordb = Chroma(embedding_function=embeddings, persist_directory=chroma_db_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inferencing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_question = \"How do I load a CSV file in LangChain?\"\n",
    "answer = answer_question(user_question, vectordb, generator)\n",
    "print(answer) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
