{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try_2 LLAMA_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install --upgrade --quiet langchain langchain-community langchainhub langchain-openai faiss-cpu 'huggingface_hub[cli,torch,tensorflow]' transformers vllm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from transformers import pipeline, AutoModel\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from tqdm import tqdm  # For progress bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, LlamaTokenizerFast, PreTrainedTokenizerFast\n",
    "\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(\"unsloth/Meta-Llama-3.1-8B-bnb-4bit\", local_files_only=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\"unsloth/Meta-Llama-3.1-8B-bnb-4bit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_langchain_docs(url, start_page=1, end_page=10):\n",
    "    \"\"\"Scrapes content from LangChain documentation pages.\"\"\"\n",
    "    all_text = []\n",
    "    for page_num in range(start_page, end_page + 1):\n",
    "        page_url = f\"{url}/p/{page_num}\"  # Adjust URL format as needed\n",
    "        response = requests.get(page_url)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Extract text content (modify selectors as needed)\n",
    "        content_div = soup.find('div', class_='markdown')\n",
    "        if content_div:\n",
    "            text = content_div.get_text(separator=\"\\n\", strip=True)\n",
    "            all_text.append(text)\n",
    "    return all_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_langchain_vectordb(docs, embeddings_model, db_path=\"langchain_docs.db\"):\n",
    "    \"\"\"Creates a ChromaDB vector database from LangChain documentation.\"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=200,\n",
    "        length_function=len\n",
    "    )\n",
    "    texts = text_splitter.create_documents(docs)\n",
    "\n",
    "    vectordb = Chroma.from_documents(\n",
    "        texts,\n",
    "        embedding=embeddings_model,\n",
    "        persist_directory=db_path\n",
    "    )\n",
    "    vectordb.persist()\n",
    "    return vectordb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = \"decapoda-research/llama-7b-hf\"  # Choose a suitable smaller model\n",
    "model_name = model_id\n",
    "generator = pipeline('text-generation', model=model_name)\n",
    "embeddings_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
    "\n",
    "def get_context_from_docs(query, vectordb):\n",
    "    \"\"\"Retrieves relevant context from LangChain documentation.\"\"\"\n",
    "    results = vectordb.similarity_search(query, k=3)\n",
    "    return [doc.page_content for doc in results]\n",
    "\n",
    "def answer_question(question, vectordb):\n",
    "    \"\"\"Answers user questions using LLM and RAG.\"\"\"\n",
    "    context = get_context_from_docs(question, vectordb)\n",
    "    prompt = f\"\"\"\n",
    "    Answer the following question based on the provided context:\n",
    "\n",
    "    Context:\n",
    "    {context}\n",
    "\n",
    "    Question: {question}\n",
    "    Answer:\n",
    "    \"\"\"\n",
    "    response = generator(prompt, max_length=200)[0]['generated_text']\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    langchain_docs_url = \"https://python.langchain.com/docs/get_started/introduction\"  # Correct URL\n",
    "    all_docs = scrape_langchain_docs(langchain_docs_url, start_page=1, end_page=5) # Scrape first 5 pages\n",
    "\n",
    "    # Create the vector database (this may take a while depending on the data)\n",
    "    vectordb = create_langchain_vectordb(all_docs, embeddings_model)\n",
    "\n",
    "    # Now you can answer questions:\n",
    "    while True:\n",
    "        user_question = input(\"Ask a question about LangChain: \")\n",
    "        answer = answer_question(user_question, vectordb)\n",
    "        print(answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
